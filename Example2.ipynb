{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "\n",
    "Let's try solve the diffusion equation\n",
    "$$\n",
    "\\dfrac{\\partial u(x,t)}{\\partial t} = \\alpha^2 \\dfrac{\\partial^2 u(x,t)}{\\partial x^2}= 0\n",
    "$$\n",
    "for $x \\in [0,1]$ and $t>0$. In order to find a solution, we need\n",
    "1. initial condition that fix  $u(x,0)=\\sin( \\pi \\ x)$\n",
    "2. boundary conditions that fix $u(0,t)=u(1,t)=0$\n",
    "\n",
    "\n",
    "Notice that the solution is  \n",
    "$$\n",
    "u(x,t) =  \\sin( \\pi \\ x) \\ e^{-(\\pi \\ \\alpha )^2 t}\n",
    "$$\n",
    "\n",
    "For simplicity, we'll set $a=\\dfrac{1}{2}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we choose\n",
    "$$\n",
    "u=w_0 \\ \\sin(w_1 \\ x) e^{ w_2 \\ t}\n",
    "$$\n",
    "\n",
    "the solution is then given for\n",
    "$$\n",
    "w_0 = \\pm 1 \\\\\n",
    "w_1 = \\mp \\pi \\\\\n",
    "w_2 = -\\left(\\dfrac{\\pi}{2}\\right)^2\n",
    "$$\n",
    "\n",
    "### Note that we have to choose a region for t. So let's will choose $0<t<2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,w0,dim_w,dim_x):\n",
    "        self.w=w0\n",
    "        self.dim_w=dim_w\n",
    "        self.dim_x=dim_x\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        return self.w[0]*np.sin(self.w[1]*x[0]) * np.exp(self.w[2]*x[1])\n",
    "    \n",
    "    def derivative_approx(self,x,h=1e-3):\n",
    "        \n",
    "        f0=0\n",
    "        f1=0\n",
    "        dfdx=[0 for _1 in range(self.dim_x)]\n",
    "\n",
    "        for i in range(self.dim_x):\n",
    "            h_eff=h+np.abs(h*x[i])\n",
    "            \n",
    "            x[i]+=h_eff\n",
    "            f1=self(x)\n",
    "            \n",
    "            x[i]+=-2*h_eff\n",
    "            f0=self(x)\n",
    "            \n",
    "            dfdx[i]=(f1-f0)/(2*h_eff)\n",
    "        return dfdx\n",
    "    \n",
    "    \n",
    "    def derivative_ij(self,i,j,x,h=1e-3):\n",
    "        h1=h+np.abs(h*x[i])\n",
    "        h2=h+np.abs(h*x[j])\n",
    "        \n",
    "        x[i]+=h1\n",
    "        x[j]+=h2\n",
    "        f_ff=self(x)\n",
    "\n",
    "        x[i]-=2*h1\n",
    "        x[j]-=2*h2\n",
    "\n",
    "        f_bb=self(x)\n",
    "        \n",
    "        x[i]+=2*h1\n",
    "\n",
    "        f_fb=self(x)\n",
    "        \n",
    "        x[i]-=2*h1\n",
    "        x[j]+=2*h2\n",
    "\n",
    "        f_bf=self(x)\n",
    "        \n",
    "        x[i]+=h1\n",
    "        x[j]-=h2\n",
    "        \n",
    "        \n",
    "        return (f_ff+f_bb-f_fb-f_bf)/(4*h1*h2)\n",
    "        \n",
    "        \n",
    "    def derivative_ii(self,i,x,h=1e-3):\n",
    "        h1=h+np.abs(h*x[i])\n",
    "\n",
    "        x[i]-=h1\n",
    "        f_b=self(x)\n",
    "        x[i]+=2*h1\n",
    "        f_f=self(x)\n",
    "        x[i]-=h1\n",
    "\n",
    "        f_0=self(x)\n",
    "\n",
    "        return (f_f+f_b-2*f_0)/(h1*h1)\n",
    "    \n",
    "    \n",
    "    def derivative_1(self,i,x,h=1e-3):\n",
    "        h1=h+np.abs(h*x[i])\n",
    "        \n",
    "        x[i]+=h1\n",
    "        f1=self(x)\n",
    "        \n",
    "        x[i]+=-2*h1\n",
    "        f0=self(x)\n",
    "        \n",
    "        x[i]+=h1\n",
    "\n",
    "        return (f1-f0)/(2*h1)\n",
    "    \n",
    "    \n",
    "    def derivative_2_approx(self,i,j,x,h=1e-3):\n",
    "        if i==j: \n",
    "            return self.derivative_ii(i,x,h)\n",
    "        else:\n",
    "            return self.derivative_ij(i,j,x,h)\n",
    "\n",
    "        \n",
    "    def derivative_1(self,i,x,h=1e-3):\n",
    "        if i==0:\n",
    "            return self.w[1]*self(x)*np.cos(self.w[1]*x[0])/np.sin(self.w[1]*x[0])\n",
    "        if i==1:\n",
    "            return self.w[2]*self(x)\n",
    "        \n",
    "\n",
    "    def derivative_2(self,i,j,x,h=1e-3):\n",
    "        if i==0 and j==0:\n",
    "            return -self.w[1]**2*self(x)\n",
    "        \n",
    "        if (i==0 and j==1) or (i==1 and j==0):\n",
    "            return self(x)*self.w[1]*self.w[2]*np.cos(self.w[1]*x[0])/np.sin(self.w[1]*x[0])\n",
    "        \n",
    "        if i==1 and j==1:\n",
    "            return self.w[2]**2*self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "guess=Model(w0=[1,.1,.2],\n",
    "                dim_w=3,\n",
    "                dim_x=2\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08173668839360554"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess([1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08146405095538067, 0.016347337678721107]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[guess.derivative_1(_,[1,-1]) for _ in range(guess.dim_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-8.187171076336582e-05, 0.016373796337629296],\n",
       " [0.016373796337629296, 0.0003274868430534633]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[guess.derivative_2(_1,_2,[.1,-1],h=1e-3) for _1 in range(guess.dim_x)] for _2 in range(guess.dim_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The boundary conditions\n",
    "\n",
    "Boundary conditions is given as in the class below. It seems to be convinient (and easy to generalize to more dimensions and more complicated boundaries) to have a function that takes a point ($\\vec{x}$) and returns a projection of this point on the boundary ($\\vec{x}_B$), then take $H(\\vec{x},f,\\partial_i f)\\Big|_{\\vec{x}=\\vec{x}_B} = 0$. \n",
    "\n",
    "Also, it would be convinient to define a function that returns a random point inside the boundary, which will be used to generate points that will be used to train the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I also think that the contribution of the boundary conditions to the loss should be included here. \n",
    "\n",
    "So, we can do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boundary:\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "        \n",
    "        \n",
    "    #get a random point in the region of interest\n",
    "    def randomPoint(self):\n",
    "        x0=np.random.rand()\n",
    "        t0=np.random.rand()*2\n",
    "        return [x0,t0]\n",
    "    \n",
    "        \n",
    "    def randomBoundaryPoints(self):\n",
    "        '''get a  point on the boundary for each boundary condition'''\n",
    "\n",
    "        x1=self.randomPoint()\n",
    "        x1[0]=0\n",
    "        \n",
    "        x2=self.randomPoint()\n",
    "        x2[0]=1\n",
    "\n",
    "        x3=self.randomPoint()\n",
    "        x3[1]=0\n",
    "\n",
    "        return [x1,x2,x3]\n",
    " \n",
    "    def randomBoundaryConditions(self):\n",
    "        \n",
    "        x1,x2,x3=self.randomBoundaryPoints()\n",
    "    \n",
    "        return [self.model(x1),self.model(x2),self.model(x3)-np.sin(np.pi*x3[0])]\n",
    "    \n",
    "    def randomBoundaryLoss(self):\n",
    "        '''average loss of the boundary conditions'''\n",
    "        \n",
    "        BC=self.randomBoundaryConditions()\n",
    "        tot=sum( b**2 for b in BC)\n",
    "        return tot/3.\n",
    "    \n",
    "    def boundaryConditions(self,xB):\n",
    "        \n",
    "        x1,x2,x3=xB\n",
    "    \n",
    "        return [self.model(x1),self.model(x2),self.model(x3)-np.sin(np.pi*x3[0])]\n",
    "    \n",
    "    def boundaryLoss(self,xB):\n",
    "        '''average loss of the boundary conditions at the points xB'''\n",
    "        x1,x2,x3=xB\n",
    "        BC = self.boundaryConditions(xB)\n",
    "        tot=sum( b**2 for b in BC)\n",
    "        return tot/3.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=Boundary(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random point:\n",
      " [0.04931014235044051, 0.31547794727413025]\n",
      "random points for each boundary condition:\n",
      " [[0, 0.1551413926365337], [1, 0.44428204504291413], [0.9951203478035227, 0]]\n",
      "boundary conditions:\n",
      " [0.0, 0.10206580085302892, -0.2911265323763792]\n",
      "\n",
      "=========================\n",
      "\n",
      "boundary conditions at\n",
      " x1= [0, 0.1]  x2= [1, 0.1]  x3= [0.2, 0] \n",
      " [0.0, 0.10185018544254351, -0.5677865855991401]\n",
      "\n",
      "=========================\n",
      "\n",
      "average loss of boundary conditions at\n",
      " x1= [0, 0.1]  x2= [1, 0.1]  x3= [0.2, 0] \n",
      " 0.11091835568700337\n"
     ]
    }
   ],
   "source": [
    "print('random point:\\n',S.randomPoint())\n",
    "print('random points for each boundary condition:\\n',S.randomBoundaryPoints())\n",
    "print('boundary conditions:\\n',S.randomBoundaryConditions())\n",
    "\n",
    "print()\n",
    "print('=========================')\n",
    "print()\n",
    "\n",
    "x=0.2\n",
    "t=0.1\n",
    "\n",
    "xB=[[0,t],[1,t],[x,0]]\n",
    "print('boundary conditions at\\n x1=',\n",
    "      xB[0],\n",
    "      ' x2=',xB[1],\n",
    "      ' x3=',xB[2],\n",
    "      '\\n',S.boundaryConditions(xB))\n",
    "\n",
    "print()\n",
    "print('=========================')\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print('average loss of boundary conditions at\\n x1=',\n",
    "      xB[0],\n",
    "      ' x2=',xB[1],\n",
    "      ' x3=',xB[2],\n",
    "      '\\n',S.boundaryLoss(xB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PDE\n",
    "\n",
    "Just define a class that holds everything that is relevant. Basically, you want have a place that can give you the loss easily.\n",
    "\n",
    "Overload the ```__call__``` function to return $\\rm lhs-rhs$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialEquation:\n",
    "    def __init__(self,Model,Boundary):\n",
    "        \n",
    "        self.model=Model\n",
    "        \n",
    "        self.Boundary=Boundary\n",
    "        \n",
    "        \n",
    "    def __call__(self,x):\n",
    "        dudt = self.model.derivative_1(1,x) \n",
    "        d2udx2 = self.model.derivative_2(0,0,x)\n",
    "        alpha=0.5\n",
    "        \n",
    "        return dudt - alpha**2 * d2udx2\n",
    "    \n",
    "    \n",
    "    def loss(self,x):\n",
    "        \n",
    "        return self(x)**2\n",
    "    \n",
    "    def averageLoss(self,x,xB):\n",
    "        \n",
    "        return (self.Boundary.boundaryLoss(xB) + self.loss(x))/2.\n",
    "    \n",
    "    \n",
    "    def randomLossGrad(self,h=1e-3):\n",
    "        '''Get the gradient of the averge loss at random point and a random boundary point'''\n",
    "        \n",
    "        x=self.Boundary.randomPoint()\n",
    "        xB=self.Boundary.randomBoundaryPoints()\n",
    "        \n",
    "        grad=[0 for i in  range(self.model.dim_w)]\n",
    "        \n",
    "        w=self.model.w[:]\n",
    "        for dim in range(self.model.dim_w):\n",
    "            h_eff=h*w[dim]+h\n",
    "            \n",
    "            self.model.w[dim]=w[dim]-h_eff\n",
    "            Q0=self.averageLoss(x,xB)\n",
    "\n",
    "            self.model.w[dim]=w[dim]+h_eff\n",
    "            Q1=self.averageLoss(x,xB)\n",
    "            \n",
    "            self.model.w[dim]=w[dim]\n",
    "            \n",
    "\n",
    "            grad[dim]=(Q1-Q0)/(2.*h_eff)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def lossGrad(self,x,xB,h=1e-3):\n",
    "        '''Get the gradient of the averge loss at random point and a random boundary point'''\n",
    "        \n",
    "        \n",
    "        grad=[0 for i in  range(self.model.dim_w)]\n",
    "        \n",
    "        w=self.model.w[:]\n",
    "        for dim in range(self.model.dim_w):\n",
    "            h_eff=h*w[dim]+h\n",
    "            \n",
    "            self.model.w[dim]=w[dim]-h_eff\n",
    "            Q0=self.averageLoss(x,xB)\n",
    "\n",
    "            self.model.w[dim]=w[dim]+h_eff\n",
    "            Q1=self.averageLoss(x,xB)\n",
    "            \n",
    "            self.model.w[dim]=w[dim]\n",
    "            \n",
    "\n",
    "            grad[dim]=(Q1-Q0)/(2.*h_eff)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this is the definiton of the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess=Model(w0=[1,.1,.2],\n",
    "                dim_w=3,\n",
    "                dim_x=2\n",
    "               )\n",
    "\n",
    "S=Boundary(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDE=DifferentialEquation(guess,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07330482234729314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it seems correct\n",
    "x=2\n",
    "t=3\n",
    "\n",
    "PDE([x,t])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05484924556530217"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it seems correct\n",
    "x=0.2\n",
    "t=-1\n",
    "\n",
    "PDE.averageLoss([x,t],[[0,t],[1,t],[x,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss should return the average of te previous two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00031010184529597917, -0.003207143778933224, 0.0004317834696732081]\n"
     ]
    }
   ],
   "source": [
    "# PDE.lossGrad([x,y])\n",
    "x=0.2\n",
    "t=0.1\n",
    "\n",
    "\n",
    "print(PDE.lossGrad([x,t],[[0,t],[1,t],[x,0]],h=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the minimum of the average loss function, using SGD!\n",
    "\n",
    "For now, I copy the NAdam from [ASAP](https://github.com/dkaramit/ASAP/tree/master/Optimization/Stochastic-Gradient-Descent/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for Stochastic Gradient Descent\n",
    "class StochasticGradientDescent:    \n",
    "    def __init__(self,strategy):\n",
    "        self.strategy=strategy\n",
    "    \n",
    "    def run(self,abs_tol=1e-5, rel_tol=1e-3, step_break=100,max_step=5000):\n",
    "        '''        \n",
    "        abs_tol, rel_tol, step_break: stop when _check<1 (_check is what update should return) \n",
    "        for step_break consecutive steps\n",
    "        \n",
    "        max_step: maximum number of steps\n",
    "        '''\n",
    "        _s=0\n",
    "        count_steps=1\n",
    "        while count_steps<=max_step:\n",
    "            _check=self.strategy.update(abs_tol, rel_tol)\n",
    "            \n",
    "            count_steps+=1             \n",
    "                \n",
    "            \n",
    "            if _check<1:\n",
    "                _s+=1\n",
    "            else:\n",
    "                _s=0\n",
    "            \n",
    "            if _s>step_break:\n",
    "                break\n",
    "\n",
    "        return self.strategy.PDE.model.w[:]\n",
    "\n",
    "    \n",
    "class NAdamSGD:\n",
    "    '''Implementation of NAdam.'''\n",
    "    \n",
    "    def __init__(self,PDE,beta_m=1-1e-1,beta_v=1-1e-3,epsilon=1e-8,alpha=1e-2):\n",
    "        '''\n",
    "        loss: the loss function\n",
    "        data: the data to be used in order to minimize the loss\n",
    "        beta_m: decay parameter for the average m\n",
    "        beta_v: decay parameter for the average v \n",
    "        epsilon: safety parameter (to avoid division by 0)\n",
    "        alpha: a learning rate that multiplies the rate of AdaDelta. \n",
    "        '''\n",
    "        self.PDE=PDE\n",
    "\n",
    "        self.beta_m=beta_m\n",
    "        self.beta_v=beta_v\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.steps=[]\n",
    "        self.steps.append(self.PDE.model.w[:])\n",
    "        self.dim=self.PDE.model.dim_w\n",
    "        \n",
    "        \n",
    "        #The \"bias corrected\" m and v need beta^iteration, so I need something like this\n",
    "        self.beta_m_ac=beta_m\n",
    "        self.beta_v_ac=beta_v\n",
    "\n",
    "        # counters for the decaying means of the gradient         \n",
    "        self.mE=[0 for _ in self.PDE.model.w]\n",
    "        self.vE=[0 for _ in self.PDE.model.w]\n",
    "        \n",
    "        #lists to store the changes in w         \n",
    "        self.dw=[0 for _ in self.PDE.model.w]\n",
    "\n",
    "    def update(self,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        update should return a number that when it is smaller than 1\n",
    "        the main loop stops. Here I choose this number to be:\n",
    "        sqrt(1/dim*sum_{i=0}^{dim}(grad/(abs_tol+x*rel_tol))_i^2)\n",
    "        '''\n",
    "        \n",
    "        grad=self.PDE.randomLossGrad()#get the loss at a random point and a random boundary point            \n",
    "        # accumulate the decay rates, in order to correct the averages \n",
    "        self.beta_m_ac*=self.beta_m_ac\n",
    "        self.beta_v_ac*=self.beta_v_ac\n",
    "        #print(grad)\n",
    "        _w2=0\n",
    "        _check=0\n",
    "        for i,g in enumerate(grad):\n",
    "            self.mE[i]=self.beta_m*self.mE[i] + (1-self.beta_m)*g \n",
    "            self.vE[i]=self.beta_v*self.vE[i] + (1-self.beta_v)*g**2\n",
    "\n",
    "            self.dw[i]=self.alpha/(np.sqrt(self.vE[i]/(1-self.beta_v_ac)) + self.epsilon)\n",
    "            self.dw[i]*=(self.beta_m*self.mE[i] + (1-self.beta_m)*g)/(1-self.beta_m_ac)\n",
    "            self.PDE.model.w[i]=self.PDE.model.w[i] - self.dw[i]\n",
    "            \n",
    "            _w2=abs_tol + self.PDE.model.w[i] * rel_tol\n",
    "            _check+=(g/_w2)*(g/_w2)\n",
    "\n",
    "        _check=np.sqrt(1./self.dim *_check)\n",
    "        \n",
    "        self.steps.append(self.PDE.model.w[:])\n",
    "        \n",
    "        return _check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess=Model(w0=[0.1,5,2],\n",
    "                dim_w=3,\n",
    "                dim_x=2\n",
    "               )\n",
    "\n",
    "S=Boundary(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDE=DifferentialEquation(guess,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy=NAdamSGD(PDE,beta_m=1-1e-1,beta_v=1-1e-3,epsilon=1e-6,alpha=1e-1)\n",
    "SGD=StochasticGradientDescent(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9998674005354811, 3.141551698069359, -2.466878167248023], 5429)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD.run(abs_tol=1e-3, rel_tol=1e-3, step_break=500,max_step=50000),len(strategy.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got w= [0.9998674005354811, 3.141551698069359, -2.466878167248023]\n",
      "I expect w= [ +-1 ,+- 3.141592653589793 , -2.4674011002723395 ]\n"
     ]
    }
   ],
   "source": [
    "#As you can see\n",
    "print('I got w=',guess.w)\n",
    "print('I expect w= [','+-1',',+-',np.pi,',',-(np.pi/2)**2,']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
