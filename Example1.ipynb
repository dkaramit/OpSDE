{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDEs and their solutions\n",
    "\n",
    "General partial differential equation I would like to solve:\n",
    "$$\n",
    "G\\Big( \\vec{x}, \\; f(\\vec{x}), \\;  \\partial_{i}f(\\vec{x}), \\; \\partial_{i}\\partial_{j}f(\\vec{x}) \\Big) = 0\n",
    "$$\n",
    "for $\\vec{x} \\in \\Omega$ (i.e. some $n$-dimentional domain).\n",
    "\n",
    "In order for this to have a solution, the following must be true (the *Cauchyâ€“Kowalevski theorem*):\n",
    "\n",
    "1.  We should be able to solve for the highest derivative. That is, there should exist an analytic $F$ so that \n",
    "$$\n",
    "\\partial_k\\partial_k f(\\vec{x})=F\\Big(\\vec{x}, f(\\vec{x}), \\partial_i f(\\vec{x}), \n",
    "\\left\\{ \\partial_i \\partial_j f(\\vec{x}) \\right\\}_{ij \\neq kk} \\Big)\n",
    "$$\n",
    "\n",
    "2. We should know:\n",
    "$$\n",
    "f(\\vec{x})\\Big|_{x_{k}=x_{k}^{(0)}} = L_{0}\\Big(\\left\\{ x_i \\right\\}_{i \\neq k}\\Big)\n",
    "\\\\\n",
    "\\partial_k f(\\vec{x})\\Big|_{x_{k}=x_{k}^{(0)}} = L_{1}\\Big(\\left\\{ x_i \\right\\}_{i \\neq k}\\Big) \n",
    "$$\n",
    "\n",
    "## Usual PDE problems\n",
    "\n",
    "In general proving that the given conditions give unique well define solution is non-trivial. However, usually, PDEs are given with some *boundary conditions* of the form\n",
    "$$\n",
    "H\\Big(\\vec{x},f(\\vec{x}),\\partial_i f(\\vec{x}) \\Big)\\Big|_{\\vec{x} \\in {\\bf S} } =0 \\;,\n",
    "$$\n",
    "with ${\\bf S}$ the boundary of the region in which we look for a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order PDEs\n",
    "\n",
    "In this notebook, we will try to solve general first order PDE inside the $n$-dimensional box. We will assume that there are boundary conditions of the form\n",
    "$$\n",
    "H\\Big(\\vec{x},f(\\vec{x}),\\partial_i f(\\vec{x}) \\Big)\\Big|_{\\vec{x} \\in {\\bf S} } =0 \\;, \n",
    "$$\n",
    "because it will be easier to generalize to 2nd order PDEs later. Probably I will have to make an object called ```Boundary```that generate points inside and on the boundary,  then I can demand that $H=0$ on the boundary points. Let's see how it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "As we build the code, let's try to solve the first order PDE\n",
    "$$\n",
    "\\dfrac{\\partial f}{\\partial x} +k \\dfrac{\\partial f}{\\partial y} = x\n",
    "$$\n",
    "in $x,y \\in [0,1]$ ($2$-dimensional box).\n",
    "\n",
    "Note that for such PDE we need \n",
    "\n",
    "1. An analytic function ($F$) such that $\\dfrac{\\partial f}{\\partial x} = F\\left(x,y,f, \\dfrac{\\partial f}{\\partial y}  \\right)$ or \n",
    "$\\dfrac{\\partial f}{\\partial y} = F\\left(x,y,f, \\dfrac{\\partial f}{\\partial x}  \\right)$  (which obviously holds).\n",
    "\n",
    "2. $f(x=x_0,y)=h(y)$ or $f(x,y=y_0)=g(x)$.\n",
    "\n",
    "\n",
    "The solution is \n",
    "$$\n",
    "f(x,y)=\\dfrac{1}{2}x^2 + (y-k \\ x) \\; c\n",
    "$$\n",
    "\n",
    "Note that the condition $f(x=x_0,y)=h(y)$ is not a boundary condition, but I still call it this, because it does not affect the code, and it will be helpful when we generalize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a general function looks-like. It depends on various free parameters (${\\bf w}$) that can be adjusted in order to solve the PDE.\n",
    "\n",
    "We will use $k=\\pi $ and $f(0,y)=\\dfrac{y}{10}$, and write the solution as\n",
    "$f(x,y)=w_0 x^2 + w_1 (y-w_2 \\ x)$. So the solution is given for\n",
    "\n",
    "$$\n",
    "w_0=\\dfrac{1}{2}\\\\\n",
    "w_1=\\dfrac{1}{10}\\\\\n",
    "w_2=\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example guess solution.\n",
    "In genearl, you can change ```__call__```, or pass it as argument, For the moment it is fine.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,w0,dim_w,dim_x):\n",
    "        self.w=w0\n",
    "        self.dim_w=dim_w\n",
    "        self.dim_x=dim_x\n",
    "        \n",
    "        \n",
    "        self.dfdx=[0 for _1 in range(self.dim_x)]\n",
    "        \n",
    "        \n",
    "    def __call__(self,x):\n",
    "        return self.w[0]*x[0]**2 + self.w[1]*(x[1]-self.w[2]*x[0])\n",
    "    \n",
    "    def derivative(self,x,h=1e-5):\n",
    "        \n",
    "        f0=0\n",
    "        f1=0\n",
    "        for i in range(self.dim_x):\n",
    "            x[i]+=h\n",
    "            f1=self(x)\n",
    "            \n",
    "            x[i]+=-2*h\n",
    "            f0=self(x)\n",
    "            \n",
    "            self.dfdx[i]=(f1-f0)/(2*h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess=Model(w0=[1,1,2],\n",
    "                dim_w=3,\n",
    "                dim_x=2\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess([10,33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess.derivative([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.000000000001]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess.dfdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The boundary conditions\n",
    "\n",
    "Boundary conditions is given as in the class below. It seems to be convinient (and easy to generalize to more dimensions and more complicated boundaries) to have a function that takes a point ($\\vec{x}$) and returns a projection of this point on the boundary ($\\vec{x}_B$), then take $H(\\vec{x},f,\\partial_i f)\\Big|_{\\vec{x}=\\vec{x}_B} = 0$. \n",
    "\n",
    "\n",
    "Also, it would be convinient to define a function that returns a random point inside the boundary, which will be used to generate points that will be used to train the model.\n",
    "\n",
    "So, we can do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boundary:\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "        \n",
    "        \n",
    "        #zero for everything, and non-zero for the condition you want for this example \n",
    "        self.BC=[[lambda x:0,lambda x:0] for _ in range(model.dim_x)]\n",
    "        self.BC[0][0]=lambda x: model([0,x[1]]) - 0.1*x[1]\n",
    "        \n",
    "    def randomPoint(self):\n",
    "        \n",
    "        return [np.random.rand() for _ in range(self.model.dim_x)]\n",
    "    \n",
    "    def boundaryPoint(self,x):\n",
    "        '''\n",
    "        Note that in this cace y can be \\pm \\infty, but I choose y \\in [0,1]\n",
    "        '''\n",
    "        xB=x[:]\n",
    "        xB[0]=0\n",
    "        return xB\n",
    "        \n",
    "    def randomBoundaryPoint(self):\n",
    "        xB=self.randomPoint()\n",
    "        xB[0]=0\n",
    "        return xB\n",
    " \n",
    "    def boundaryCondition(self,x):\n",
    "        \n",
    "        xB=self.boundaryPoint(x)\n",
    "    \n",
    "        return self.model(xB) - 0.1*xB[1]\n",
    "    \n",
    "    def randomBoundaryCondition(self):\n",
    "        return self.boundaryCondition(self.randomPoint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=Boundary(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8,\n",
       " ([0, 2], [0.1965543661376924, 0.4033083671383062], [0, 0.23294903759227836]),\n",
       " 0.6735088872596295]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[S.boundaryCondition([555,2]),\n",
    "(S.boundaryPoint([5,2]),S.randomPoint(),S.randomBoundaryPoint()),\n",
    "S.randomBoundaryCondition()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The coefficient matrix\n",
    "\n",
    "The general form of a 1st order PDE can be written as\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n} A_{i}\\left(\\vec{x},  f(\\vec{x}) \\right) \\ \\partial_i f(\\vec{x}) = {\\rm RHS}(\\vec{x},f(\\vec{x})) \\;,\n",
    "$$\n",
    "\n",
    "with $A_{i}\\left(\\vec{x},  f(\\vec{x}) \\right)$ a known coefficient $n$-column, and ${\\rm RHS}(\\vec{x},f(\\vec{x}))$ the right-hand-side of the PDE.\n",
    "\n",
    "\n",
    "For the case under study, we have (let's take $k=\\pi$)\n",
    "\n",
    "$$\n",
    "A = \\Bigg(\\begin{matrix} 1 \\\\ \\pi \\end{matrix}\\Bigg) \\\\\n",
    "{\\rm RHS}=x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use classes becauseI will generalize it to second order PDEs, and I will have to get the derivatives of the model.\n",
    "\n",
    "class Coefficient:\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "        \n",
    "        self.coeff=[lambda x:0 for _ in range(self.model.dim_x)]\n",
    "        \n",
    "        self.coeff[0]=lambda x: 1\n",
    "        self.coeff[1]=lambda x: np.pi\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        return [c(x) for c in self.coeff]\n",
    "\n",
    "    \n",
    "    \n",
    "class RightHandSide:\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        \n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=Coefficient(guess)\n",
    "RHS=RightHandSide(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3.141592653589793]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RHS([5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put everything together, and define a class that describes the differential equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialEquation:\n",
    "    def __init__(self,Model,Coefficient,RightHandSide,Boundary):\n",
    "        \n",
    "        self.model=Model\n",
    "        \n",
    "        self.A=Coefficient\n",
    "        \n",
    "        self.RHS=RightHandSide\n",
    "        \n",
    "        self.Boundary=Boundary\n",
    "        \n",
    "        \n",
    "    def LHS(self,x,h=1e-5):\n",
    "        lhs=0\n",
    "        self.model.derivative(x,h)\n",
    "        dfdx=self.model.dfdx\n",
    "        A=self.A(x)\n",
    "        for i  in range(self.model.dim_x):\n",
    "            lhs+=A[i]*dfdx[i]\n",
    "        \n",
    "        return lhs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss(self,x):\n",
    "        # loss at x\n",
    "        Qx=(self.LHS(x)-self.RHS(x))**2\n",
    "        return Qx\n",
    "        \n",
    "    def lossBC(self,x):        \n",
    "        # Mean loss at the boundary. \n",
    "        return self.Boundary.boundaryCondition(x)**2\n",
    "    \n",
    "    def averageLoss(self,x):\n",
    "        # averaged loss (loss at the boundary + loss at x)/2 \n",
    "        return (self.lossBC(x) + self.loss(x))/2.\n",
    "    \n",
    "    def randomAverageLoss():\n",
    "        #average loss at random points\n",
    "        x=Boundary.randomPoint()\n",
    "        xB=Boundary.randomBoundaryPoint()\n",
    "        \n",
    "        return (self.loss(x)+self.lossBC(xB))/2.\n",
    "    \n",
    "    def lossGrad(self,x,h=1e-3):\n",
    "        '''\n",
    "        with the definition of h_eff, h=1e-3 should be enough.\n",
    "        In any case, the idea is to compute the derivetive analytically in general.\n",
    "        '''\n",
    "        grad=[0 for i in  range(self.model.dim_w)]\n",
    "        \n",
    "        w=self.model.w[:]\n",
    "        for dim in range(self.model.dim_w):\n",
    "            h_eff=h*w[dim]+h\n",
    "            \n",
    "            self.model.w[dim]=w[dim]-h_eff\n",
    "            Q0=self.averageLoss(x)\n",
    "\n",
    "            self.model.w[dim]=w[dim]+h_eff\n",
    "            Q1=self.averageLoss(x)\n",
    "            \n",
    "            self.model.w[dim]=w[dim]\n",
    "            \n",
    "\n",
    "            grad[dim]=(Q1-Q0)/(2.*h_eff)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def randomLossGrad(self,h=1e-3):\n",
    "        '''Get the gradient of the averge loss at random point and a random boundary point'''\n",
    "        \n",
    "        x=self.Boundary.randomPoint()\n",
    "        xB=self.Boundary.randomBoundaryPoint()\n",
    "        \n",
    "        grad=[0 for i in  range(self.model.dim_w)]\n",
    "        \n",
    "        w=self.model.w[:]\n",
    "        for dim in range(self.model.dim_w):\n",
    "            h_eff=h*w[dim]+h\n",
    "            \n",
    "            self.model.w[dim]=w[dim]-h_eff\n",
    "            Q0=(self.loss(x)+self.lossBC(xB))/2.\n",
    "\n",
    "            self.model.w[dim]=w[dim]+h_eff\n",
    "            Q1=(self.loss(x)+self.lossBC(xB))/2.\n",
    "            \n",
    "            self.model.w[dim]=w[dim]\n",
    "            \n",
    "\n",
    "            grad[dim]=(Q1-Q0)/(2.*h_eff)\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this is the definiton of the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess=Model(w0=[1,1,2],\n",
    "                dim_w=3,\n",
    "                dim_x=2\n",
    "               )\n",
    "\n",
    "S=Boundary(guess)\n",
    "\n",
    "A=Coefficient(guess)\n",
    "RHS=RightHandSide(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDE=DifferentialEquation(guess,A,RHS,S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that ${\\rm LHS} = 2w_0 x - w_1 w_2 +\\pi w_1$ and ${\\rm RHS}=x$.\n",
    "\n",
    "Our current values are $w_0 = 1$, $w_1 = 1$ and $w_2 = 2$. That is ${\\rm LHS}=2x-2+\\pi$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.141592653636579, 7.141592653589793)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it seems correct\n",
    "x=3\n",
    "y=2\n",
    "\n",
    "PDE.LHS([x,y]),  2*PDE.model.w[0]*x-PDE.model.w[1]*PDE.model.w[2]+np.pi*PDE.model.w[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the loss is \n",
    "$$\n",
    "\\left({\\rm LHS} - {\\rm RHS}\\right)^2=\\left( 2w_0 x - w_1 w_2 +\\pi w_1 -x \\right)^2 = \n",
    "\\left( x - 2 + \\pi \\right)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss at the boundary (as defined by our convention before) is:\n",
    "$$\n",
    "\\left(f(0,y)-\\dfrac{1}{10}y\\right)^2=\\left(w_1 y -\\dfrac{1}{10}y\\right)^2= y^2 (w_1 - 0.1)^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.152872540609547 17.152789708268944\n",
      "3.24 3.24\n"
     ]
    }
   ],
   "source": [
    "#it seems correct\n",
    "\n",
    "x=3\n",
    "y=2\n",
    "\n",
    "print(PDE.loss([x,y]), (2*PDE.model.w[0]*x-PDE.model.w[1]*PDE.model.w[2]+np.pi*PDE.model.w[1] - x)**2)\n",
    "\n",
    "print(PDE.lossBC([x,y]), y**2*(0.1-PDE.model.w[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss should return the average of te previous two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.196436270304773, 10.196436270304773)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#again correct!\n",
    "PDE.averageLoss([x,y]), (PDE.loss([x,y])+PDE.lossBC([x,y]))/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the average loss (defined by our convention) is:\n",
    "$$\n",
    "\\dfrac{\\partial Q}{\\partial w_0} = 2x\\left( 2w_0 x - w_1 w_2 +\\pi w_1 -x \\right) \\\\\n",
    "\\dfrac{\\partial Q}{\\partial w_1} = (\\pi-w_2)\\left( 2w_0 x - w_1 w_2 +\\pi w_1 -x \\right) + y^2 (w_1 - 0.1) \\\\\n",
    "\\dfrac{\\partial Q}{\\partial w_2} = -w_1 \\left( 2w_0 x - w_1 w_2 +\\pi w_1 -x \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.835140466266115, 8.313500299419907, -4.1511601956161215]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.84955592153876, 8.328011747499565, -4.141592653589793]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDE.lossGrad([x,y])\n",
    "\n",
    "print(PDE.lossGrad([x,y]))\n",
    "[\n",
    "2*x*(2*PDE.model.w[0]*x-PDE.model.w[1]*PDE.model.w[2]+np.pi*PDE.model.w[1]-x),   \n",
    "(np.pi - PDE.model.w[2]) * (2*PDE.model.w[0]*x-PDE.model.w[1]*PDE.model.w[2]+np.pi*PDE.model.w[1]-x)+\n",
    "y**2*(PDE.model.w[1] -0.1),\n",
    "-PDE.model.w[1]*(2*PDE.model.w[0]*x-PDE.model.w[1]*PDE.model.w[2]+np.pi*PDE.model.w[1]-x)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the minimum of the average loss function, using SGD!\n",
    "\n",
    "For now, I copy the NAdam from [ASAP](https://github.com/dkaramit/ASAP/tree/master/Optimization/Stochastic-Gradient-Descent/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for Stochastic Gradient Descent\n",
    "class StochasticGradientDescent:    \n",
    "    def __init__(self,strategy):\n",
    "        self.strategy=strategy\n",
    "    \n",
    "    def run(self,abs_tol=1e-5, rel_tol=1e-3, step_break=100,max_step=5000):\n",
    "        '''        \n",
    "        abs_tol, rel_tol, step_break: stop when _check<1 (_check is what update should return) \n",
    "        for step_break consecutive steps\n",
    "        \n",
    "        max_step: maximum number of steps\n",
    "        '''\n",
    "        _s=0\n",
    "        count_steps=1\n",
    "        while count_steps<=max_step:\n",
    "            _check=self.strategy.update(abs_tol, rel_tol)\n",
    "            \n",
    "            count_steps+=1             \n",
    "                \n",
    "            \n",
    "            if _check<1:\n",
    "                _s+=1\n",
    "            else:\n",
    "                _s=0\n",
    "            \n",
    "            if _s>step_break:\n",
    "                break\n",
    "\n",
    "        return self.strategy.PDE.model.w[:]\n",
    "\n",
    "    \n",
    "class NAdamSGD:\n",
    "    '''Implementation of NAdam.'''\n",
    "    \n",
    "    def __init__(self,PDE,beta_m=1-1e-1,beta_v=1-1e-3,epsilon=1e-8,alpha=1e-2):\n",
    "        '''\n",
    "        loss: the loss function\n",
    "        data: the data to be used in order to minimize the loss\n",
    "        beta_m: decay parameter for the average m\n",
    "        beta_v: decay parameter for the average v \n",
    "        epsilon: safety parameter (to avoid division by 0)\n",
    "        alpha: a learning rate that multiplies the rate of AdaDelta. \n",
    "        '''\n",
    "        self.PDE=PDE\n",
    "\n",
    "        self.beta_m=beta_m\n",
    "        self.beta_v=beta_v\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.steps=[]\n",
    "        self.steps.append(self.PDE.model.w[:])\n",
    "        self.dim=self.PDE.model.dim_w\n",
    "        \n",
    "        \n",
    "        #The \"bias corrected\" m and v need beta^iteration, so I need something like this\n",
    "        self.beta_m_ac=beta_m\n",
    "        self.beta_v_ac=beta_v\n",
    "\n",
    "        # counters for the decaying means of the gradient         \n",
    "        self.mE=[0 for _ in self.PDE.model.w]\n",
    "        self.vE=[0 for _ in self.PDE.model.w]\n",
    "        \n",
    "        #lists to store the changes in w         \n",
    "        self.dw=[0 for _ in self.PDE.model.w]\n",
    "\n",
    "    def update(self,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        update should return a number that when it is smaller than 1\n",
    "        the main loop stops. Here I choose this number to be:\n",
    "        sqrt(1/dim*sum_{i=0}^{dim}(grad/(abs_tol+x*rel_tol))_i^2)\n",
    "        '''\n",
    "        \n",
    "        grad=self.PDE.randomLossGrad()#get the loss at a random point and a random boundary point            \n",
    "        # accumulate the decay rates, in order to correct the averages \n",
    "        self.beta_m_ac*=self.beta_m_ac\n",
    "        self.beta_v_ac*=self.beta_v_ac\n",
    "        \n",
    "        _w2=0\n",
    "        _check=0\n",
    "        for i,g in enumerate(grad):\n",
    "            self.mE[i]=self.beta_m*self.mE[i] + (1-self.beta_m)*g \n",
    "            self.vE[i]=self.beta_v*self.vE[i] + (1-self.beta_v)*g**2\n",
    "\n",
    "            self.dw[i]=self.alpha/(np.sqrt(self.vE[i]/(1-self.beta_v_ac)) + self.epsilon)\n",
    "            self.dw[i]*=(self.beta_m*self.mE[i] + (1-self.beta_m)*g)/(1-self.beta_m_ac)\n",
    "            self.PDE.model.w[i]=self.PDE.model.w[i] - self.dw[i]\n",
    "            \n",
    "            _w2=abs_tol + self.PDE.model.w[i] * rel_tol\n",
    "            _check+=(g/_w2)*(g/_w2)\n",
    "\n",
    "        _check=np.sqrt(1./self.dim *_check)\n",
    "        \n",
    "        self.steps.append(self.PDE.model.w[:])\n",
    "        \n",
    "        return _check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy=NAdamSGD(PDE,beta_m=1-1e-1,beta_v=1-1e-3,epsilon=1e-8,alpha=1e-2)\n",
    "SGD=StochasticGradientDescent(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.49964216512503046, 0.09999914712193492, 3.137527006000515], 5245)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD.run(abs_tol=1e-5, rel_tol=1e-3, step_break=150,max_step=50000),len(strategy.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got w= [0.49964216512503046, 0.09999914712193492, 3.137527006000515]\n",
      "I expected w= [0.5, 0.1, 3.141592653589793]\n"
     ]
    }
   ],
   "source": [
    "#As you can see\n",
    "print('I got w=',guess.w)\n",
    "print('I expected w=',[1/2.,1/10.,np.pi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already said, the solution is given for $w_0=\\dfrac{1}{2},\\; w_1=\\dfrac{1}{10},\\; w_2=\\pi$.\n",
    "That is, the thing works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
